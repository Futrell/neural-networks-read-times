{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word frequency statistics for control features\n",
    "word_freq = Counter()\n",
    "with open(\"../data/wikitext-2_train_vocab.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        token, freq = line.strip().split(\"\\t\")\n",
    "        word_freq[token] = int(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harmonize lists of <(word, int),(word, int)> pairs\n",
    "# Discards pairs where words do not match\n",
    "\n",
    "punct_at_end_re = re.compile(r\"\\W+$\")\n",
    "punct_at_start_re = re.compile(r\"^\\W+\")\n",
    "\n",
    "contractions_re = re.compile(r\"([^' ])('ll|'LL|'re|'RE|'ve|'VE|n't|N'T|not|NOT|'[sS]|'[mM]|'[dD]|')\")\n",
    "\n",
    "mismatches = Counter()\n",
    "biggest_mismatch_code, _ = mismatches_old.most_common(1)[0]\n",
    "\n",
    "def harmonize_rows(ref, d):\n",
    "    result = []\n",
    "    curr_d = d.pop(0)\n",
    "    curr_ref = ref.pop(0)\n",
    "    \n",
    "    to_print = 0\n",
    "    mismatched = [0, None]\n",
    "    while len(d) > 10:\n",
    "        model_token, surprisal = curr_d\n",
    "        code, rt_token, rt = curr_ref\n",
    "        \n",
    "        if punct_at_start_re.search(rt_token):\n",
    "            to_print = 10\n",
    "            \n",
    "        if to_print > 0:\n",
    "            #print(\"\\t\", code, model_token, surprisal, rt_token)\n",
    "            to_print -= 1\n",
    "            if to_print == 0:\n",
    "                pass\n",
    "                #print(\"=======\")\n",
    "                \n",
    "        if code > biggest_mismatch_code - 5 and code < biggest_mismatch_code + 5:\n",
    "            print(\"\\t\", code if code != biggest_mismatch_code else \"**\" + str(code), model_token, surprisal, rt_token)\n",
    "                \n",
    "        if mismatched[0] == 5:\n",
    "            mismatches[mismatched[1]] += 1\n",
    "        \n",
    "        #print(curr_d[2] + \" \" + curr_ref[0])\n",
    "        if model_token == rt_token:\n",
    "            #print(\"===\" + curr_d[2] + \"-\" + curr_ref[0])\n",
    "            result.append(curr_d + curr_ref)\n",
    "            curr_d = d.pop(0)\n",
    "            curr_ref = ref.pop(0)\n",
    "            mismatched = [0, None]\n",
    "        else:\n",
    "            if mismatched[0] == 0:\n",
    "                mismatched = [1, code]\n",
    "            else:\n",
    "                mismatched[0] += 1\n",
    "            # If current token is unked, then pop both\n",
    "            if \"UNK\" in model_token:\n",
    "                curr_d = d.pop(0)\n",
    "                curr_ref = ref.pop(0)\n",
    "            # If current ref has both leading and trailing punctuation, remove and re-check\n",
    "            elif punct_at_start_re.search(rt_token) and punct_at_end_re.search(rt_token):\n",
    "                rt_token_new = punct_at_start_re.sub(\"\", punct_at_end_re.sub(\"\", rt_token))\n",
    "                curr_ref = (code, rt_token_new, rt)\n",
    "                \n",
    "                # If current model token(s) are that punctuation, drop those tokens\n",
    "                match = punct_at_start_re.findall(rt_token)[0]\n",
    "                while match.startswith(model_token):\n",
    "                    match = match[len(model_token):]\n",
    "                    model_token, surprisal = d.pop(0)\n",
    "\n",
    "                curr_d = (model_token, surprisal)\n",
    "                \n",
    "                # If next model token(s) are that punctuation, drop those tokens\n",
    "                match = punct_at_end_re.findall(rt_token)[0]\n",
    "                print(\"-----\", rt_token, match, d[0][0])\n",
    "                while match.startswith(d[0][0]):\n",
    "                    match = match[len(d[0][0]):]\n",
    "                    d.pop(0)\n",
    "            # If current ref has trailing punctuation, remove and re-check\n",
    "            elif punct_at_end_re.search(rt_token):\n",
    "                rt_token_new = punct_at_end_re.sub(\"\", rt_token)\n",
    "                curr_ref = (code, rt_token_new, rt)\n",
    "\n",
    "                # If next model token(s) are that punctuation, drop those tokens\n",
    "                match = punct_at_end_re.findall(rt_token)[0]\n",
    "                while match.startswith(d[0][0]):\n",
    "                    match = match[len(d[0][0]):]\n",
    "                    d.pop(0)\n",
    "            # If current ref has leading punctuation, remove and re-check\n",
    "            elif punct_at_start_re.search(rt_token):\n",
    "                rt_token_new = punct_at_start_re.sub(\"\", rt_token)\n",
    "                curr_ref = (code, rt_token_new, rt)\n",
    "\n",
    "                # If current model token(s) are that punctuation, drop those tokens\n",
    "                match = punct_at_start_re.findall(rt_token)[0]\n",
    "                while match.startswith(model_token):\n",
    "                    match = match[len(model_token):]\n",
    "                    model_token, surprisal = d.pop(0)\n",
    "\n",
    "                curr_d = (model_token, surprisal)\n",
    "            # de-tokenize PTB splits of contractions\n",
    "            elif contractions_re.search(rt_token):\n",
    "                ideal_tokenized_form = tuple(contractions_re.sub(r\"\\1 \\2\", rt_token).split(\" \"))\n",
    "\n",
    "                # Make sure we have the expanded form here in the model tokenization\n",
    "                future_model_tokens = d[:len(ideal_tokenized_form) - 1]\n",
    "                model_token_full = [model_token] + [tok for tok, _ in future_model_tokens]\n",
    "                if ideal_tokenized_form == tuple(model_token_full):\n",
    "                    # Build a little synthetic `curr_d`, `curr_ref` by adding surprisals\n",
    "                    curr_d = (\"\".join(model_token_full), surprisal + sum(surp for _, surp in future_model_tokens))\n",
    "                    d = d[len(future_model_tokens):]\n",
    "                else:\n",
    "                    curr_d = d.pop(0)\n",
    "            # If current ref has leading punctuation, pop both\n",
    "            elif not rt_token.isalpha():\n",
    "                curr_ref = ref.pop(0)\n",
    "                curr_d = d.pop(0)\n",
    "            #If the current word is the end of a line\n",
    "            elif \"EOL\" in rt_token:\n",
    "                curr_ref = ref.pop(0)\n",
    "                curr_d = d.pop(0)\n",
    "            else:\n",
    "                curr_d = d.pop(0)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbeb8e5a22134e6591bf39d8608fca16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Harmonizing models', max=4.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f247a390e04384bda78e1af01aeb75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Test files', max=80.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54dc427282844c68e286c5b94aa47b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Test files', max=4.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- (red, , ,\n",
      "----- 'You, , UNK-CAPS\n",
      "----- 'No, , '\n",
      "----- (red, , ,\n",
      "----- (red, , ,\n",
      "----- (red, , ,\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43fc1058d6794be5bbc812375779ad30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Test files', max=180.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6829eec33df44dff95b912394686e27c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Test files', max=9.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 'You, , UNK-CAPS\n",
      "----- 'No, , '\n",
      "----- (red, , ,\n",
      "----- 'You, , UNK-CAPS\n",
      "----- 'No, , '\n",
      "----- (red, , ,\n",
      "----- (red, , ,\n",
      "----- (red, , ,\n",
      "----- 'You, , UNK-CAPS\n",
      "----- 'No, , '\n",
      "----- (red, , ,\n",
      "----- (red, , ,\n",
      "----- 'You, , UNK-CAPS\n",
      "----- 'No, , '\n",
      "----- (red, , ,\n",
      "----- (red, , ,\n",
      "----- (red, , ,\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb008b762bd458397d689efd370d17e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Test files', max=180.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b98c9a5b094b329342fc2b085fc52f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Test files', max=9.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- (red, , ,\n",
      "----- 'You, , UNK-CAPS\n",
      "----- 'No, , '\n",
      "----- (red, , ,\n",
      "----- (red, , ,\n",
      "----- 'You, , UNK-CAPS\n",
      "----- 'No, , '\n",
      "----- (red, , ,\n",
      "----- (red, , ,\n",
      "----- (red, , ,\n",
      "----- 'You, , UNK-CAPS\n",
      "----- 'No, , '\n",
      "----- (red, , ,\n",
      "----- (red, , ,\n",
      "----- (red, , ,\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0565fb3a85442cc810669b742350280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Test files', max=140.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec5c01636504a01b4edc00d4e4624e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Test files', max=7.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- (red, , ,\n",
      "----- 'fog'. '. '\n",
      "----- (red, , ,\n",
      "----- 'fog'. '. '\n",
      "----- 'fog'. '. '\n",
      "----- 'fog'. '. '\n",
      "----- 'fog'. '. '\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def merge_model_results():\n",
    "    \n",
    "    final_df = []\n",
    "    \n",
    "    models = [f for f in os.listdir(\"../data/model_results\") if not f.startswith(\".\")]\n",
    "    for m in tqdm(models, desc=\"Harmonizing models\"):\n",
    "        #tqdm.write(\"Harmonizing results for \" + m)\n",
    "        test_corpus = [f for f in os.listdir(\"../data/model_results/\" + m) if not f.startswith(\".\")]\n",
    "        for tc in test_corpus:\n",
    "            test_files = [f for f in os.listdir(\"../data/model_results/\" + m + \"/\" + tc) if not f.startswith(\".\")]\n",
    "            \n",
    "            for tf in tqdm(test_files, desc=\"Test files\"):\n",
    "                if tf == \"UNKS\":\n",
    "                    print(\"TODO: UNKS\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    tf = tf.split(\"_\")\n",
    "                    test_filename = tf[0]\n",
    "                    model_architecture = tf[1]\n",
    "                    training_data = tf[2]\n",
    "                    seed = tf[3].replace(\".csv\", \"\")\n",
    "                except:\n",
    "                    print(tf)\n",
    "                    \n",
    "                if tc == \"dundee\":\n",
    "                    continue\n",
    "                \n",
    "                # Special handling for the Dundee corpus\n",
    "                if tc == \"dundee\":\n",
    "                    gold_test_filename = test_filename.replace(\"wrdp\", \"\") + \"_avg\"\n",
    "                    gold_standard = pd.read_csv(\"../data/human_rts/\" + tc + \"/\" + gold_test_filename + \".txt\", sep=\"\\t\", names=[\"word\", \"surprisal\"])\n",
    "                    gold_standard.insert(0, 'code', range(0,len(gold_standard)))\n",
    "                    gold_standard[\"code\"] = gold_standard[\"code\"] + int(test_filename.replace(\"tx\", \"\").replace(\"wrdp\", \"\")) * 10000\n",
    "                else:\n",
    "                    gold_standard = pd.read_csv(\"../data/human_rts/\" + tc + \"/\" + test_filename + \".txt\", sep=\"\\t\")\n",
    "                                    \n",
    "                model_results = \"_\".join([test_filename, model_architecture, training_data, seed])\n",
    "                model_path = \"/\".join([\"../data/model_results\", m, tc, model_results])\n",
    "                model_results = pd.read_csv(model_path+\".csv\", sep=\"\\t\")\n",
    "                \n",
    "                # PTB-de-process model results\n",
    "                model_results.token = model_results.token.str.replace(\"-LRB-\", \"(\")\n",
    "                model_results.token = model_results.token.str.replace(\"-RRB-\", \")\")\n",
    "            \n",
    "                # TODO: EOL Handleing\n",
    "                \n",
    "                model_results = [tuple(x)[2:4] for x in model_results.values.tolist()]\n",
    "                gold_standard = [tuple(x) for x in gold_standard.values.tolist()]\n",
    "                \n",
    "                harmonized_results = harmonize_rows(gold_standard, model_results)\n",
    "                \n",
    "                result = [tuple((x[2], x[0], x[1], x[4], tc, model_architecture, training_data, seed, len(x[0]), math.log(word_freq[x[0]]+1))) for x in harmonized_results]\n",
    "                final_df.extend(result)\n",
    "                \n",
    "    df = pd.DataFrame(final_df)\n",
    "    df.columns = [\"code\", \"word\", \"surprisal\", \"psychometric\", \"corpus\", \"model\", \"training\", \"seed\", \"len\", \"freq\"]\n",
    "    df.head()\n",
    "    df.to_csv(\"../data/harmonized_results.csv\")\n",
    "    return df\n",
    "\n",
    "df = merge_model_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(60391, 24),\n",
       " (10776, 20),\n",
       " (60394, 20),\n",
       " (70140, 14),\n",
       " (20277, 13),\n",
       " (20515, 8),\n",
       " (40199, 8),\n",
       " (40745, 8),\n",
       " (50428, 8),\n",
       " (60735, 8),\n",
       " (60945, 8),\n",
       " (70164, 8),\n",
       " (70247, 8),\n",
       " (70595, 8),\n",
       " (70643, 8),\n",
       " (80142, 8),\n",
       " (100302, 8),\n",
       " (100311, 8),\n",
       " (100321, 8),\n",
       " (20516, 6)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import copy\n",
    "mismatches_old = copy(mismatches)\n",
    "mismatches_old.most_common()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
