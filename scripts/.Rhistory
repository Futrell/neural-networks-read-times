library(tidyverse)
library(lme4)
library(lmerTest)
library(plotrix)
library(stringr)
library(readxl)
library(RColorBrewer)
library(viridis)
library(Hmisc)
library(mvtnorm)
library(mgcv)
# Compute the log-likelihood of a new dataset using a fit lme4 model.
logLik_test <- function(lm, test_X, test_y) {
predictions <- predict(lm, test_X, re.form=NA)
# Get std.dev. of residual, estimated from train data
stdev <- sigma(lm)
# For each prediction--observation, get the density p(obs | N(predicted, model_sigma)) and reduce
density <- sum(dnorm(test_y, predictions, stdev, log=TRUE))
return(density)
}
# Get per-prediction log-likelihood
logLik_test_per <- function(lm, test_X, test_y) {
predictions <- predict(lm, test_X, re.form=NA)
# Get std.dev. of residual, estimated from train data
stdev <- sigma(lm)
# For each prediction--observation, get the density p(obs | N(predicted, model_sigma))
densities <- dnorm(test_y, predictions, stdev, log=TRUE)
return(densities)
}
# Compute MSE of a new dataset using a fit lme4 model.
mse_test <- function(lm, test_X, test_y) {
return(mean((predict(lm, test_X, re.form=NA) - test_y) ^ 2))
}
#Sanity checks
#mylm <- gam(psychometric ~  s(surprisal, bs = "cr", k = 20) + s(prev_surp, bs = "cr", k = 20) + te(freq, len, bs = "cr") + te(prev_freq, prev_len, bs = "cr"), data=train_data)
#c(logLik(mylm), logLik_test(mylm, train_data, train_data$psychometric))
#logLik_test(mylm, test_data, test_data$psychometric)
data = read.csv("../data/harmonized_results.csv")
all_data = data %>%
mutate(seed = as.factor(seed)) %>%
group_by(corpus, model, training, seed) %>%
mutate(prev_surp = lag(surprisal),
prev_code = lag(code),
prev_len = lag(len),
prev_freq = lag(freq),
prev_surp = lag(surprisal),
prev2_freq = lag(prev_freq),
prev2_code = lag(prev_code),
prev2_len = lag(prev_len),
prev2_surp = lag(prev_surp),
prev3_freq = lag(prev2_freq),
prev3_code = lag(prev2_code),
prev3_len = lag(prev2_len),
prev3_surp = lag(prev2_surp)) %>%
ungroup() %>%
# Filter back three for the dundee corpus. Filter back 1 for all other corpora
filter((corpus == "dundee" & code == prev2_code + 2) | (corpus != "dundee" & code == prev_code + 1)) %>%
select(-prev_code, -prev2_code, -prev3_code) %>%
drop_na()
# Compute linear model stats for the given training data subset and full test data.
# Automatically subsets the test data to match the relevant group for which we are training a linear model.
get_lm_data <- function(df, test_data, formula, store_env) {
#this_lm <- gam(formula, data=df);
this_lm = lm(formula, data=df)
this_test_data <- semi_join(test_data, df, by=c("training", "model", "seed", "corpus"));
# Save lm to the global env so that we can access residuals later.
lm_name = unique(paste(df$model, df$training, df$seed, df$corpus))[1]
assign(lm_name, this_lm, envir=store_env)
summarise(df,
log_lik = as.numeric(logLik(this_lm, REML = F)),
test_lik = logLik_test(this_lm, this_test_data, this_test_data$psychometric),
test_mse = mse_test(this_lm, this_test_data, this_test_data$psychometric))
}
# For a previously fitted lm stored in store_env, get the residuals on test data of the relevant data subset.
get_lm_residuals <- function(df, store_env) {
# Retrieve the relevant lm.
lm_name = unique(paste(df$model, df$training, df$seed, df$corpus))[1]
this_lm <- get(lm_name, envir=store_env)
mutate(df,
likelihood = logLik_test_per(this_lm, df, df$psychometric),
resid = df$psychometric - predict(this_lm, df, re.form=NA))
}
#####
# Define regression formulae.
# Eye-tracking regression: only use surprisal and previous surprisal; SPRT regression: use 2-back features.
#baseline_rt_regression = psychometric ~ te(freq, len, bs = "cr") + te(prev_freq, prev_len, bs = "cr")
#baselie_sprt_regression = psychometric ~ te(freq, len, bs = "cr") + te(prev_freq, prev_len, bs = "cr") + te(prev2_freq, prev2_len, bs = "cr")
#full_rt_regression = (psychometric ~ s(surprisal, bs = "cr", k = 20) + s(prev_surp, bs = "cr", k = 20)
#+ te(freq, len, bs = "cr") + te(prev_freq, prev_len, bs = "cr"))
#full_sprt_regression = (psychometric ~ s(surprisal, bs = "cr", k = 20) + s(prev_surp, bs = "cr", k = 20) + s(prev2_surp, bs = "cr", k = 20)
#+ te(freq, len, bs = "cr") + te(prev_freq, prev_len, bs = "cr") + te(prev2_freq, prev2_len, bs = "cr"))
baseline_rt_regression = psychometric ~ freq + prev_freq + prev2_freq + prev3_freq + len + prev_len + prev2_len + prev3_len
baseline_sprt_regression = psychometric ~ freq + prev_freq + len + prev_len
full_rt_regression = psychometric ~ surprisal + prev_surp + prev2_surp + prev3_surp + freq + prev_freq + prev2_freq + prev3_freq + len + prev_len + prev2_len + prev3_len
full_sprt_regression = psychometric ~ surprisal + prev_surp + freq + prev_freq + len + prev_len
#####
# Prepare frames/environments for storing results/objects.
baseline_results = data.frame()
full_model_results = data.frame()
baseline_residuals = data.frame()
full_residuals = data.frame()
#Randomly shuffle the data
all_data<-all_data[sample(nrow(all_data)),]
#Create K equally size folds
K = 5
folds <- cut(seq(1,nrow(all_data)),breaks=K,labels=FALSE)
#Perform 10 fold cross validation
baseline_corpus = function(corpus, df, test_data, env) {
if(corpus == "dundee") {
get_lm_data(df, test_data, baseline_rt_regression, env)
} else {
get_lm_data(df, test_data, baseline_sprt_regression, env)
}
}
full_model_corpus = function(corpus, df, test_data, env) {
if(corpus[1] == "dundee") {
get_lm_data(df, test_data, full_rt_regression, env)
} else {
get_lm_data(df, test_data, full_sprt_regression, env)
}
}
for(i in 1:K) {
#Segement your data by fold using the which() function
testIndexes <- which(folds==i,arr.ind=TRUE)
test_data <- all_data[testIndexes, ]
train_data <- all_data[-testIndexes, ]
# Prepare a new Environment in which we store fitted LMs, which we'll query later for residuals.
baseline_env = environment()
full_env = environment()
# Compute a baseline linear model for each model--training--seed--RT-corpus combination.
baselines = train_data %>%
group_by(model, training, seed, corpus) %>%
print(model) %>%
do(baseline_corpus(unique(.$corpus), ., test_data, baseline_env)) %>%
ungroup() %>%
mutate(seed = as.factor(seed),
fold = i)
baseline_results = rbind(baseline_results, baselines)
# Compute a full linear model for each model--training--seed-RT-corpus combination
full_models = train_data %>%
group_by(model, training, seed, corpus) %>%
do(full_model_corpus(unique(.$corpus), ., test_data, full_env)) %>%
ungroup() %>%
mutate(seed = as.factor(seed),
fold = i)
full_model_results = rbind(full_model_results, full_models)
fold_baseline_residuals = test_data %>%
group_by(model, training, seed, corpus) %>%
do(get_lm_residuals(., baseline_env)) %>%
ungroup()
baseline_residuals = rbind(baseline_residuals, fold_baseline_residuals)
fold_full_residuals = test_data %>%
group_by(model, training, seed, corpus) %>%
do(get_lm_residuals(., full_env)) %>%
ungroup()
full_residuals = rbind(full_residuals, fold_full_residuals)
}
#write.csv(full_residuals, "../data/full_residuals.csv")
#write.csv(baseline_residuals, "../data/baseline_residuals.csv")
#write.csv(full_model_results, "../data/full_model_result.csv")
#write.csv(baseline_results, "../data/baseline_results.csv")
#full_model_results = read.csv("../data/full_model_results.csv")
#baseline_results = read.csv("../data/baseline_resultsb.csv")
# Join baseline models with full models and compare performance within-fold.
model_fold_deltas = baseline_results %>%
right_join(full_model_results, suffix=c(".baseline", ".full"),
by=c("model", "training", "seed", "corpus", "fold")) %>%
mutate(seed = as.factor(seed)) %>%
# Compute per-fold deltas.
group_by(model, training, seed, corpus, fold) %>%
mutate(delta_log_lik = test_lik.full - test_lik.baseline,
delta_mse = test_mse.full - test_mse.baseline) %>%
ungroup() %>%
select(model, training, seed, corpus, fold,
delta_log_lik, delta_mse)
# Now compute across-fold delta statistics for each model--training--seed--corpus.
model_deltas = model_fold_deltas %>%
group_by(model, training, seed, corpus) %>%
summarise(mean_delta_log_lik = sum(delta_log_lik),
sem_delta_log_lik = sd(delta_log_lik) / sqrt(length(delta_log_lik)),
mean_delta_mse = sum(delta_mse),
sem_delta_mse = sd(delta_mse) / sqrt(length(delta_mse)))
metric <- "ΔLogLik"
#metric <- "-ΔMSE"
# Select the relevant metric.
model_fold_deltas = model_fold_deltas %>%
# Retrieve the current test metric
mutate(delta_test = delta_log_lik) %>%
select(-delta_log_lik, -delta_mse)
# Select the relevant metric.
model_deltas = model_deltas %>%
# Retrieve the current test metric
mutate(delta_test_mean = mean_delta_log_lik,
delta_test_sem = sem_delta_log_lik) %>%
# mutate(delta_test_mean = mean_delta_mse,
#        delta_test_sem = sem_delta_mse)
# Remove the raw metrics.
select(-mean_delta_log_lik, -sem_delta_log_lik,
-mean_delta_mse, -sem_delta_mse)
model_deltas
# Sanity check: training on train+test data should yield improved performance over training on just training data. (When evaluating on test data.)
# full_baselines = all_data %>%
#   group_by(model, training, seed, corpus) %>%
#   summarise(baseline_train_all_test_lik = logLik_test(lm(psychometric ~ len + freq + sent_pos, data=.), semi_join(test_data, ., by=c("training", "model", "seed", "corpus")), semi_join(test_data, ., by=c("training", "model", "seed", "corpus"))$psychometric)) %>%
#   ungroup()
# full_baselines
#
# full_baselines %>%
#   right_join(baselines, by=c("seed", "training", "model", "corpus")) %>%
#   mutate(delta=baseline_train_all_test_lik-baseline_test_lik) %>%
#   select(-baseline_lik) # %>%
#   #select(-baseline_test_lik, -baseline_train_all_test_lik, -baseline_lik, -baseline_test_mse)
language_model_data = read.csv("./data/model_metadata.csv") %>%
mutate(train_size = case_when(training == "bllip-lg" ~ 42,
training == "bllip-md" ~ 15,
training == "bllip-sm" ~ 5,
training == "bllip-xs" ~ 1)) %>%
mutate(seed = as.factor(seed)) %>%
select(-X, -corpus, -model_key, -mse, -corr, -l1, -train_l1) %>%
distinct(model, training, seed, .keep_all = TRUE)
language_model_data = read.csv("./data/model_metadata.csv") %>%
mutate(train_size = case_when(training == "bllip-lg" ~ 42,
training == "bllip-md" ~ 15,
training == "bllip-sm" ~ 5,
training == "bllip-xs" ~ 1)) %>%
mutate(seed = as.factor(seed)) %>%
select(-X, -corpus, -model_key, -mse, -corr, -l1, -train_l1) %>%
distinct(model, training, seed, .keep_all = TRUE)
language_model_data = read.csv("./data/model_metadata.csv") %>%
mutate(train_size = case_when(training == "bllip-lg" ~ 42,
training == "bllip-md" ~ 15,
training == "bllip-sm" ~ 5,
training == "bllip-xs" ~ 1)) %>%
mutate(seed = as.factor(seed)) %>%
select(-X, -corpus, -model_key, -mse, -corr, -l1, -train_l1) %>%
distinct(model, training, seed, .keep_all = TRUE)
language_model_data = read.csv("../data/model_metadata.csv") %>%
mutate(train_size = case_when(training == "bllip-lg" ~ 42,
training == "bllip-md" ~ 15,
training == "bllip-sm" ~ 5,
training == "bllip-xs" ~ 1)) %>%
mutate(seed = as.factor(seed)) %>%
select(-X, -corpus, -model_key, -mse, -corr, -l1, -train_l1) %>%
distinct(model, training, seed, .keep_all = TRUE)
# TODO: Why are the seeds and models not the same in the two data frames?
# I think we have some 5gram seeds with an extra "1" in the factor name. This is borking things.
# TODO check with jenn/ethan about this
table(language_model_data$seed)
table(model_deltas$seed)
model_deltas = model_deltas %>%
right_join(language_model_data, by=c("seed", "training", "model")) %>%
drop_na()
model_deltas
# Exclude ordered-neurons from all analyses.
model_deltas <- model_deltas %>%
filter(model != "ordered-neurons",
corpus != "bnc-brown")
model_fold_deltas <- model_fold_deltas %>%
filter(model != "ordered-neurons")
model_deltas %>%
ggplot(aes(x=sg_score, y=delta_test_mean)) +
geom_errorbar(aes(ymin=delta_test_mean-delta_test_sem, ymax=delta_test_mean+delta_test_sem)) +
geom_smooth(method="lm", se=T) +
geom_point(stat="identity", position="dodge", alpha=1, size=3, aes(color=training, shape=model)) +
ylab(metric) +
xlab("Syntax Generalization Score") +
ggtitle("Syntactic Generalization vs. Predictive Power") +
#scale_color_manual(values = c("#A42EF1", "#3894C8")) +
facet_grid(~corpus, scales="free") +
theme(axis.text=element_text(size=14),
strip.text.x = element_text(size=14),
legend.text=element_text(size=14),
axis.title=element_text(size=18),
legend.position = "bottom")
#ggsave("./cogsci_images/sg_loglik.png",height=5,width=6)
# Prepare a residualized regression for x1 onto y, controlling for the effects of x2.
d_resid = model_fold_deltas %>%
drop_na() %>%
# Residualize delta metric w.r.t PPL for each model--training--seed--fold
group_by(corpus) %>%
mutate(resid.delta = resid(lm(delta_test ~ training:test_ppl))) %>%
ungroup() %>%
# Residualize SG score w.r.t. PPL for each training group
group_by(training) %>%
# NB no need for training:ppl interaction, since we're within-group.
mutate(resid.sg = resid(lm(sg_score ~ test_ppl))) %>%
ungroup() %>%
# Compute summary statistics across model--training--seed--corpus.
group_by(model, training, corpus, seed) %>%
summarise(resid.delta.mean = mean(resid.delta),
resid.delta.sem = sd(resid.delta) / sqrt(length(resid.delta)),
resid.sg.mean = mean(resid.sg),
resid.sg.sem = sd(resid.sg) / sqrt(length(resid.sg)))
model_deltas %>%
ggplot(aes(x=test_ppl, y=delta_test_mean, color=training)) +
geom_errorbar(aes(ymin=delta_test_mean-delta_test_sem, ymax=delta_test_mean+delta_test_sem), alpha=0.4) +
#geom_smooth(method="lm", se=F) +
geom_point(stat="identity", position="dodge", alpha=1, size=4, aes(shape=model)) +
ylab(metric) +
xlab("Test Perplexity") +
#coord_cartesian(ylim = c(1, 16)) +
ggtitle("Test Perplexity vs. Predictive Power") +
scale_color_manual(values = c("#440154FF", "#39568CFF", "#1F968BFF", "#73D055FF")) +
facet_grid(~corpus, scales="free") +
#coord_cartesian(ylim = c(0, 150)) +
theme(axis.text=element_text(size=12),
strip.text.x = element_text(size=12),
legend.text=element_text(size=12),
axis.title=element_text(size=12),
legend.position = "right")
#ggsave("./cogsci_images/ppl_loglik.png",height=4.5,width=11)
all_data %>%
filter(surprisal < 15, surprisal > 0) %>%
ggplot(aes(x=surprisal, y=psychometric, color=training)) +
stat_smooth(se=T, alpha=0.5) +
#geom_errorbar(color="black", width=.2, position=position_dodge(width=.9), alpha=0.3) +
#geom_point(stat="identity", position="dodge", alpha=1, size=3) +
ylab("Processing Time (ms)") +
xlab("Surprisal (bits)") +
ggtitle("Surprisal vs. Reading Time / Gaze Duration") +
facet_grid(corpus~model, scales = "free") +
scale_color_manual(values = c("#440154FF", "#39568CFF", "#1F968BFF", "#73D055FF")) +
theme(axis.text=element_text(size=14),
axis.text.y = element_text(size = 10),
strip.text.x = element_text(size=14),
legend.text=element_text(size=14),
axis.title=element_text(size=18),
legend.position = "right")
#ggsave("./cogsci_images/surp_corr.png",height=4.5,width=11)
