sem_delta_mse = sd(delta_mse) / sqrt(length(delta_mse)))
metric <- "ΔLogLik"
#metric <- "-ΔMSE"
# Select the relevant metric.
model_fold_deltas = model_fold_deltas %>%
# Retrieve the current test metric
mutate(delta_test = delta_log_lik) %>%
select(-delta_log_lik, -delta_mse)
# Select the relevant metric.
model_deltas = model_deltas %>%
# Retrieve the current test metric
mutate(delta_test_mean = mean_delta_log_lik,
delta_test_sem = sem_delta_log_lik) %>%
# mutate(delta_test_mean = mean_delta_mse,
#        delta_test_sem = sem_delta_mse)
# Remove the raw metrics.
select(-mean_delta_log_lik, -sem_delta_log_lik,
-mean_delta_mse, -sem_delta_mse)
model_deltas
language_model_data = read.csv("../data/full_ppl.csv") %>%
mutate(train_size = case_when(training == "bllip-lg" ~ 42,
training == "bllip-md" ~ 15,
training == "bllip-sm" ~ 5,
training == "bllip-xs" ~ 1)) %>%
mutate(seed = as.factor(seed)) %>%
select(-pid, -test_loss) %>%
#select(-X, -corpus, -model_key, -mse, -corr, -l1, -train_l1) %>%
distinct(model, training, seed, .keep_all = TRUE)
# TODO: Why are the seeds and models not the same in the two data frames?
# I think we have some 5gram seeds with an extra "1" in the factor name. This is borking things.
# TODO check with jenn/ethan about this
table(language_model_data$seed)
table(model_deltas$seed)
language_model_data
model_deltas = model_deltas %>%
right_join(language_model_data, by=c("seed", "training", "model")) %>%
drop_na()
model_deltas
# Exclude ordered-neurons from all analyses.
model_deltas <- model_deltas %>%
filter(model != "ordered-neurons",
corpus != "bnc-brown")
model_fold_deltas <- model_fold_deltas %>%
filter(model != "ordered-neurons")
model_deltas %>%
ggplot(aes(x=test_ppl, y=delta_test_mean, color=training)) +
geom_errorbar(aes(ymin=delta_test_mean-delta_test_sem, ymax=delta_test_mean+delta_test_sem), alpha=0.4) +
#geom_smooth(method="lm", se=F) +
geom_point(stat="identity", position="dodge", alpha=1, size=4, aes(shape=model)) +
ylab(metric) +
xlab("Test Perplexity") +
#coord_cartesian(ylim = c(1, 16)) +
ggtitle("Test Perplexity vs. Predictive Power") +
scale_color_manual(values = c("#440154FF", "#39568CFF", "#1F968BFF", "#73D055FF")) +
facet_grid(~corpus, scales="free") +
#coord_cartesian(ylim = c(0, 150)) +
theme(axis.text=element_text(size=12),
strip.text.x = element_text(size=12),
legend.text=element_text(size=12),
axis.title=element_text(size=12),
legend.position = "right")
#ggsave("./cogsci_images/ppl_loglik.png",height=4.5,width=11)
language_model_data
model_deltas
#write.csv(full_model_results, "../data/full_model_result.csv")
#write.csv(baseline_results, "../data/baseline_results.csv")
#full_model_results = read.csv("../data/full_model_results.csv")
#baseline_results = read.csv("../data/baseline_resultsb.csv")
# Join baseline models with full models and compare performance within-fold.
model_fold_deltas = baseline_results %>%
right_join(full_model_results, suffix=c(".baseline", ".full"),
by=c("model", "training", "seed", "corpus", "fold")) %>%
mutate(seed = as.factor(seed)) %>%
# Compute per-fold deltas.
group_by(model, training, seed, corpus, fold) %>%
mutate(delta_log_lik = test_lik.full - test_lik.baseline,
delta_mse = test_mse.full - test_mse.baseline) %>%
ungroup() %>%
select(model, training, seed, corpus, fold,
delta_log_lik, delta_mse)
# Now compute across-fold delta statistics for each model--training--seed--corpus.
model_deltas = model_fold_deltas %>%
group_by(model, training, seed, corpus) %>%
summarise(mean_delta_log_lik = sum(delta_log_lik),
sem_delta_log_lik = sd(delta_log_lik) / sqrt(length(delta_log_lik)),
mean_delta_mse = sum(delta_mse),
sem_delta_mse = sd(delta_mse) / sqrt(length(delta_mse)))
metric <- "ΔLogLik"
#metric <- "-ΔMSE"
# Select the relevant metric.
model_fold_deltas = model_fold_deltas %>%
# Retrieve the current test metric
mutate(delta_test = delta_log_lik) %>%
select(-delta_log_lik, -delta_mse)
# Select the relevant metric.
model_deltas = model_deltas %>%
# Retrieve the current test metric
mutate(delta_test_mean = mean_delta_log_lik,
delta_test_sem = sem_delta_log_lik) %>%
# mutate(delta_test_mean = mean_delta_mse,
#        delta_test_sem = sem_delta_mse)
# Remove the raw metrics.
select(-mean_delta_log_lik, -sem_delta_log_lik,
-mean_delta_mse, -sem_delta_mse)
model_deltas
metric <- "ΔLogLik"
#metric <- "-ΔMSE"
# Select the relevant metric.
model_fold_deltas = model_fold_deltas %>%
# Retrieve the current test metric
mutate(delta_test = delta_log_lik) %>%
select(-delta_log_lik, -delta_mse)
metric <- "ΔLogLik"
#metric <- "-ΔMSE"
# Select the relevant metric.
model_fold_deltas = model_fold_deltas %>%
# Retrieve the current test metric
mutate(delta_test = delta_log_lik) %>%
select(-delta_log_lik, -delta_mse)
#write.csv(full_model_results, "../data/full_model_result.csv")
#write.csv(baseline_results, "../data/baseline_results.csv")
#full_model_results = read.csv("../data/full_model_results.csv")
#baseline_results = read.csv("../data/baseline_resultsb.csv")
# Join baseline models with full models and compare performance within-fold.
model_fold_deltas = baseline_results %>%
right_join(full_model_results, suffix=c(".baseline", ".full"),
by=c("model", "training", "seed", "corpus", "fold")) %>%
mutate(seed = as.factor(seed)) %>%
# Compute per-fold deltas.
group_by(model, training, seed, corpus, fold) %>%
mutate(delta_log_lik = test_lik.full - test_lik.baseline,
delta_mse = test_mse.full - test_mse.baseline) %>%
ungroup() %>%
select(model, training, seed, corpus, fold,
delta_log_lik, delta_mse)
# Now compute across-fold delta statistics for each model--training--seed--corpus.
model_deltas = model_fold_deltas %>%
group_by(model, training, seed, corpus) %>%
summarise(mean_delta_log_lik = sum(delta_log_lik),
sem_delta_log_lik = sd(delta_log_lik) / sqrt(length(delta_log_lik)),
mean_delta_mse = sum(delta_mse),
sem_delta_mse = sd(delta_mse) / sqrt(length(delta_mse)))
metric <- "ΔLogLik"
#metric <- "-ΔMSE"
# Select the relevant metric.
model_fold_deltas = model_fold_deltas %>%
# Retrieve the current test metric
mutate(delta_test = delta_log_lik) %>%
select(-delta_log_lik, -delta_mse)
# Select the relevant metric.
model_deltas = model_deltas %>%
# Retrieve the current test metric
mutate(delta_test_mean = mean_delta_log_lik,
delta_test_sem = sem_delta_log_lik) %>%
# mutate(delta_test_mean = mean_delta_mse,
#        delta_test_sem = sem_delta_mse)
# Remove the raw metrics.
select(-mean_delta_log_lik, -sem_delta_log_lik,
-mean_delta_mse, -sem_delta_mse)
model_deltas
language_model_data = read.csv("../data/full_ppl.csv") %>%
mutate(train_size = case_when(training == "bllip-lg" ~ 42,
training == "bllip-md" ~ 15,
training == "bllip-sm" ~ 5,
training == "bllip-xs" ~ 1)) %>%
mutate(seed = as.factor(seed)) %>%
select(-pid, -test_loss) %>%
#select(-X, -corpus, -model_key, -mse, -corr, -l1, -train_l1) %>%
distinct(model, training, seed, .keep_all = TRUE)
# TODO: Why are the seeds and models not the same in the two data frames?
# I think we have some 5gram seeds with an extra "1" in the factor name. This is borking things.
# TODO check with jenn/ethan about this
table(language_model_data$seed)
table(model_deltas$seed)
language_model_data
model_deltas
model_deltas1 = model_deltas %>%
right_join(language_model_data, by=c("seed", "training", "model")) %>%
drop_na()
model_deltas1
model_deltas1 = model_deltas %>%
right_join(language_model_data, by=c("seed", "training", "model"))
model_deltas1
language_model_data
model_deltas
model_deltas1 = model_deltas %>%
merge(language_model_data, by = c("seed", "training", "model")) %>%
#right_join(language_model_data, by=c("seed", "training", "model")) %>%
drop_na()
model_deltas1
model_deltas1 = model_deltas %>%
merge(language_model_data, by = c("seed", "training", "model")) %>%
#right_join(language_model_data, by=c("seed", "training", "model")) %>%
#drop_na()
model_deltas1
model_deltas1 = model_deltas %>%
merge(language_model_data, by = c("seed", "training", "model"))
#right_join(language_model_data, by=c("seed", "training", "model")) %>%
#drop_na()
model_deltas1
model_deltas1 = model_deltas %>%
merge(language_model_data, by = c("seed", "training", "model"), all=T)
#right_join(language_model_data, by=c("seed", "training", "model")) %>%
#drop_na()
model_deltas1
language_model_data
model_deltas
language_model_data
model_deltas1 = model_deltas %>%
merge(language_model_data, by = c("seed", "training", "model"), all=T)
model_deltas1
language_model_data = read.csv("../data/full_ppl.csv") %>%
mutate(model = if_else(model == "gpt-2", "gpt2", model)) %>%
mutate(train_size = case_when(training == "bllip-lg" ~ 42,
training == "bllip-md" ~ 15,
training == "bllip-sm" ~ 5,
training == "bllip-xs" ~ 1)) %>%
mutate(seed = as.factor(seed)) %>%
select(-pid, -test_loss) %>%
#select(-X, -corpus, -model_key, -mse, -corr, -l1, -train_l1) %>%
distinct(model, training, seed, .keep_all = TRUE)
language_model_data = read.csv("../data/full_ppl.csv") %>%
mutate(model = as.character(model),
model = if_else(model == "gpt-2", "gpt2", model),
model = as.factor(model)) %>%
mutate(train_size = case_when(training == "bllip-lg" ~ 42,
training == "bllip-md" ~ 15,
training == "bllip-sm" ~ 5,
training == "bllip-xs" ~ 1)) %>%
mutate(seed = as.factor(seed)) %>%
select(-pid, -test_loss) %>%
#select(-X, -corpus, -model_key, -mse, -corr, -l1, -train_l1) %>%
distinct(model, training, seed, .keep_all = TRUE)
# TODO: Why are the seeds and models not the same in the two data frames?
# I think we have some 5gram seeds with an extra "1" in the factor name. This is borking things.
# TODO check with jenn/ethan about this
table(language_model_data$seed)
table(model_deltas$seed)
model_deltas1 = model_deltas %>%
merge(language_model_data, by = c("seed", "training", "model"), all=T)
#right_join(language_model_data, by=c("seed", "training", "model")) %>%
#drop_na()
model_deltas1
model_deltas1 = model_deltas %>%
merge(language_model_data, by = c("seed", "training", "model"), all=T)
#right_join(language_model_data, by=c("seed", "training", "model")) %>%
drop_na()
model_deltas1 = model_deltas %>%
merge(language_model_data, by = c("seed", "training", "model"), all=T) %>%
#right_join(language_model_data, by=c("seed", "training", "model")) %>%
drop_na()
model_deltas1
# Exclude ordered-neurons from all analyses.
model_deltas <- model_deltas %>%
filter(model != "ordered-neurons",
corpus != "bnc-brown")
model_fold_deltas <- model_fold_deltas %>%
filter(model != "ordered-neurons")
model_deltas %>%
ggplot(aes(x=test_ppl, y=delta_test_mean, color=training)) +
geom_errorbar(aes(ymin=delta_test_mean-delta_test_sem, ymax=delta_test_mean+delta_test_sem), alpha=0.4) +
#geom_smooth(method="lm", se=F) +
geom_point(stat="identity", position="dodge", alpha=1, size=4, aes(shape=model)) +
ylab(metric) +
xlab("Test Perplexity") +
#coord_cartesian(ylim = c(1, 16)) +
ggtitle("Test Perplexity vs. Predictive Power") +
scale_color_manual(values = c("#440154FF", "#39568CFF", "#1F968BFF", "#73D055FF")) +
facet_grid(~corpus, scales="free") +
#coord_cartesian(ylim = c(0, 150)) +
theme(axis.text=element_text(size=12),
strip.text.x = element_text(size=12),
legend.text=element_text(size=12),
axis.title=element_text(size=12),
legend.position = "right")
model_deltas
model_deltas = model_deltas %>%
merge(language_model_data, by = c("seed", "training", "model"), all=T) %>%
#right_join(language_model_data, by=c("seed", "training", "model")) %>%
drop_na()
model_deltas
model_deltas %>%
ggplot(aes(x=test_ppl, y=delta_test_mean, color=training)) +
geom_errorbar(aes(ymin=delta_test_mean-delta_test_sem, ymax=delta_test_mean+delta_test_sem), alpha=0.4) +
#geom_smooth(method="lm", se=F) +
geom_point(stat="identity", position="dodge", alpha=1, size=4, aes(shape=model)) +
ylab(metric) +
xlab("Test Perplexity") +
#coord_cartesian(ylim = c(1, 16)) +
ggtitle("Test Perplexity vs. Predictive Power") +
scale_color_manual(values = c("#440154FF", "#39568CFF", "#1F968BFF", "#73D055FF")) +
facet_grid(~corpus, scales="free") +
#coord_cartesian(ylim = c(0, 150)) +
theme(axis.text=element_text(size=12),
strip.text.x = element_text(size=12),
legend.text=element_text(size=12),
axis.title=element_text(size=12),
legend.position = "right")
#ggsave("./cogsci_images/ppl_loglik.png",height=4.5,width=11)
#write.csv(full_model_results, "../data/full_model_result.csv")
#write.csv(baseline_results, "../data/baseline_results.csv")
#full_model_results = read.csv("../data/full_model_results.csv")
#baseline_results = read.csv("../data/baseline_resultsb.csv")
# Join baseline models with full models and compare performance within-fold.
model_fold_deltas = baseline_results %>%
right_join(full_model_results, suffix=c(".baseline", ".full"),
by=c("model", "training", "seed", "corpus", "fold")) %>%
mutate(seed = as.factor(seed)) %>%
# Compute per-fold deltas.
group_by(model, training, seed, corpus, fold) %>%
mutate(delta_log_lik = test_lik.full - test_lik.baseline,
delta_mse = test_mse.full - test_mse.baseline) %>%
ungroup() %>%
select(model, training, seed, corpus, fold,
delta_log_lik, delta_mse)
# Now compute across-fold delta statistics for each model--training--seed--corpus.
model_deltas = model_fold_deltas %>%
group_by(model, training, seed, corpus) %>%
summarise(mean_delta_log_lik = sum(delta_log_lik),
sem_delta_log_lik = sd(delta_log_lik) / sqrt(length(delta_log_lik)),
mean_delta_mse = sum(delta_mse),
sem_delta_mse = sd(delta_mse) / sqrt(length(delta_mse)))
metric <- "ΔLogLik"
#metric <- "-ΔMSE"
# Select the relevant metric.
model_fold_deltas = model_fold_deltas %>%
# Retrieve the current test metric
mutate(delta_test = delta_log_lik) %>%
select(-delta_log_lik, -delta_mse)
# Select the relevant metric.
model_deltas = model_deltas %>%
# Retrieve the current test metric
mutate(delta_test_mean = mean_delta_log_lik,
delta_test_sem = sem_delta_log_lik) %>%
# mutate(delta_test_mean = mean_delta_mse,
#        delta_test_sem = sem_delta_mse)
# Remove the raw metrics.
select(-mean_delta_log_lik, -sem_delta_log_lik,
-mean_delta_mse, -sem_delta_mse)
model_deltas
metric <- "ΔLogLik"
#metric <- "-ΔMSE"
# Select the relevant metric.
model_fold_deltas = model_fold_deltas %>%
# Retrieve the current test metric
mutate(delta_test = delta_log_lik) %>%
select(-delta_log_lik, -delta_mse)
#write.csv(full_model_results, "../data/full_model_result.csv")
#write.csv(baseline_results, "../data/baseline_results.csv")
#full_model_results = read.csv("../data/full_model_results.csv")
#baseline_results = read.csv("../data/baseline_resultsb.csv")
# Join baseline models with full models and compare performance within-fold.
model_fold_deltas = baseline_results %>%
right_join(full_model_results, suffix=c(".baseline", ".full"),
by=c("model", "training", "seed", "corpus", "fold")) %>%
mutate(seed = as.factor(seed)) %>%
# Compute per-fold deltas.
group_by(model, training, seed, corpus, fold) %>%
mutate(delta_log_lik = test_lik.full - test_lik.baseline,
delta_mse = test_mse.full - test_mse.baseline) %>%
ungroup() %>%
select(model, training, seed, corpus, fold,
delta_log_lik, delta_mse)
# Now compute across-fold delta statistics for each model--training--seed--corpus.
model_deltas = model_fold_deltas %>%
group_by(model, training, seed, corpus) %>%
summarise(mean_delta_log_lik = sum(delta_log_lik),
sem_delta_log_lik = sd(delta_log_lik) / sqrt(length(delta_log_lik)),
mean_delta_mse = sum(delta_mse),
sem_delta_mse = sd(delta_mse) / sqrt(length(delta_mse)))
metric <- "ΔLogLik"
#metric <- "-ΔMSE"
# Select the relevant metric.
model_fold_deltas = model_fold_deltas %>%
# Retrieve the current test metric
mutate(delta_test = delta_log_lik) %>%
select(-delta_log_lik, -delta_mse)
# Select the relevant metric.
model_deltas = model_deltas %>%
# Retrieve the current test metric
mutate(delta_test_mean = mean_delta_log_lik,
delta_test_sem = sem_delta_log_lik) %>%
# mutate(delta_test_mean = mean_delta_mse,
#        delta_test_sem = sem_delta_mse)
# Remove the raw metrics.
select(-mean_delta_log_lik, -sem_delta_log_lik,
-mean_delta_mse, -sem_delta_mse)
model_deltas
language_model_data = read.csv("../data/full_ppl.csv") %>%
mutate(model = as.character(model),
model = if_else(model == "gpt-2", "gpt2", model),
model = as.factor(model)) %>%
mutate(train_size = case_when(training == "bllip-lg" ~ 42,
training == "bllip-md" ~ 15,
training == "bllip-sm" ~ 5,
training == "bllip-xs" ~ 1)) %>%
mutate(seed = as.factor(seed)) %>%
select(-pid, -test_loss) %>%
#select(-X, -corpus, -model_key, -mse, -corr, -l1, -train_l1) %>%
distinct(model, training, seed, .keep_all = TRUE)
# TODO: Why are the seeds and models not the same in the two data frames?
# I think we have some 5gram seeds with an extra "1" in the factor name. This is borking things.
# TODO check with jenn/ethan about this
table(language_model_data$seed)
table(model_deltas$seed)
model_deltas = model_deltas %>%
merge(language_model_data, by = c("seed", "training", "model"), all=T) %>%
#right_join(language_model_data, by=c("seed", "training", "model")) %>%
drop_na()
model_deltas
#write.csv(full_model_results, "../data/full_model_result.csv")
#write.csv(baseline_results, "../data/baseline_results.csv")
#full_model_results = read.csv("../data/full_model_results.csv")
#baseline_results = read.csv("../data/baseline_resultsb.csv")
# Join baseline models with full models and compare performance within-fold.
model_fold_deltas = baseline_results %>%
right_join(full_model_results, suffix=c(".baseline", ".full"),
by=c("model", "training", "seed", "corpus", "fold")) %>%
mutate(seed = as.factor(seed)) %>%
# Compute per-fold deltas.
group_by(model, training, seed, corpus, fold) %>%
mutate(delta_log_lik = test_lik.full - test_lik.baseline,
delta_mse = test_mse.full - test_mse.baseline) %>%
ungroup() %>%
select(model, training, seed, corpus, fold,
delta_log_lik, delta_mse)
# Now compute across-fold delta statistics for each model--training--seed--corpus.
model_deltas = model_fold_deltas %>%
group_by(model, training, seed, corpus) %>%
summarise(mean_delta_log_lik = sum(delta_log_lik),
sem_delta_log_lik = sd(delta_log_lik) / sqrt(length(delta_log_lik)),
mean_delta_mse = sum(delta_mse),
sem_delta_mse = sd(delta_mse) / sqrt(length(delta_mse)))
metric <- "ΔLogLik"
#metric <- "-ΔMSE"
# Select the relevant metric.
model_fold_deltas = model_fold_deltas %>%
# Retrieve the current test metric
mutate(delta_test = delta_log_lik) %>%
select(-delta_log_lik, -delta_mse)
# Select the relevant metric.
model_deltas = model_deltas %>%
# Retrieve the current test metric
mutate(delta_test_mean = mean_delta_log_lik,
delta_test_sem = sem_delta_log_lik) %>%
# mutate(delta_test_mean = mean_delta_mse,
#        delta_test_sem = sem_delta_mse)
# Remove the raw metrics.
select(-mean_delta_log_lik, -sem_delta_log_lik,
-mean_delta_mse, -sem_delta_mse)
model_deltas
language_model_data = read.csv("../data/full_ppl.csv") %>%
mutate(model = as.character(model),
model = if_else(model == "gpt-2", "gpt2", model),
model = as.factor(model)) %>%
mutate(train_size = case_when(training == "bllip-lg" ~ 42,
training == "bllip-md" ~ 15,
training == "bllip-sm" ~ 5,
training == "bllip-xs" ~ 1)) %>%
mutate(seed = as.factor(seed)) %>%
select(-pid, -test_loss) %>%
#select(-X, -corpus, -model_key, -mse, -corr, -l1, -train_l1) %>%
distinct(model, training, seed, .keep_all = TRUE)
# TODO: Why are the seeds and models not the same in the two data frames?
# I think we have some 5gram seeds with an extra "1" in the factor name. This is borking things.
# TODO check with jenn/ethan about this
table(language_model_data$seed)
table(model_deltas$seed)
model_deltas = model_deltas %>%
merge(language_model_data, by = c("seed", "training", "model"), all=T) %>%
#right_join(language_model_data, by=c("seed", "training", "model")) %>%
drop_na()
model_deltas
# Exclude ordered-neurons from all analyses.
model_deltas <- model_deltas %>%
filter(model != "ordered-neurons",
corpus != "bnc-brown")
model_fold_deltas <- model_fold_deltas %>%
filter(model != "ordered-neurons")
model_deltas %>%
ggplot(aes(x=sg_score, y=delta_test_mean)) +
geom_errorbar(aes(ymin=delta_test_mean-delta_test_sem, ymax=delta_test_mean+delta_test_sem)) +
geom_smooth(method="lm", se=T) +
geom_point(stat="identity", position="dodge", alpha=1, size=3, aes(color=training, shape=model)) +
ylab(metric) +
xlab("Syntax Generalization Score") +
ggtitle("Syntactic Generalization vs. Predictive Power") +
#scale_color_manual(values = c("#A42EF1", "#3894C8")) +
facet_grid(~corpus, scales="free") +
theme(axis.text=element_text(size=14),
strip.text.x = element_text(size=14),
legend.text=element_text(size=14),
axis.title=element_text(size=18),
legend.position = "bottom")
model_deltas %>%
ggplot(aes(x=test_ppl, y=delta_test_mean, color=training)) +
geom_errorbar(aes(ymin=delta_test_mean-delta_test_sem, ymax=delta_test_mean+delta_test_sem), alpha=0.4) +
#geom_smooth(method="lm", se=F) +
geom_point(stat="identity", position="dodge", alpha=1, size=4, aes(shape=model)) +
ylab(metric) +
xlab("Test Perplexity") +
#coord_cartesian(ylim = c(1, 16)) +
ggtitle("Test Perplexity vs. Predictive Power") +
scale_color_manual(values = c("#440154FF", "#39568CFF", "#1F968BFF", "#73D055FF")) +
facet_grid(~corpus, scales="free") +
#coord_cartesian(ylim = c(0, 150)) +
theme(axis.text=element_text(size=12),
strip.text.x = element_text(size=12),
legend.text=element_text(size=12),
axis.title=element_text(size=12),
legend.position = "right")
#ggsave("./cogsci_images/ppl_loglik.png",height=4.5,width=11)
